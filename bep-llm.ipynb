{
 "cells": [
  {
   "cell_type": "code",
   "id": "4789aaf3acea2c91",
   "metadata": {},
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import lmstudio as lms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "print('Configuring...', end='', flush=True)\n",
    "\n",
    "data_string = 'data/data.ods'\n",
    "sheet_string = 'Thematic Analysis + ASAQ'\n",
    "pd.set_option('display.max_columns', None)  # show all columns\n",
    "pd.set_option('display.width', None)  # don't break lines\n",
    "pd.set_option('display.max_colwidth', None)  # show full content in each cell\n",
    "\n",
    "# columns: 'agentName', 'EXP', 'AGE', 'AGEGROUP', 'SEX', 'REGION.OF.COUNTRY', 'EDUCATION'\n",
    "df = pd.read_excel(data_string, sheet_name=sheet_string)\n",
    "\n",
    "# Phi, Deepseek, Mistral, LLama, Qwen, Gemma\n",
    "llm_models = ['llama-3.1-8b-instruct', 'qwen3-32b', 'deepseek-r1-distill-qwen-32b', 'phi-4', 'gemma-3-12b', 'mistral-nemo-instruct-2407']\n",
    "\n",
    "SERVER_API_HOST = \"localhost:1234\"\n",
    "lms.configure_default_client(SERVER_API_HOST)\n",
    "\n",
    "print('Done! ✅', flush=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98133f8b4a0d5594",
   "metadata": {},
   "source": [
    "# Given the text, give me the themes\n",
    "text = '\\n'.join(df['EXP'].astype(str))\n",
    "\n",
    "chat = lms.Chat(f\"\"\"You are an expert qualitative researcher conducting a thematic analysis to answer the Research Question: \"How do people experience their interaction with Artificial Social Agents?\"\n",
    "\n",
    "Please perform a full thematic analysis of the following text, following these three steps carefully:\n",
    "\n",
    "Step 1: Familiarization\n",
    "- Read the entire text thoroughly.\n",
    "- Provide a concise summary of the overall topics, key impressions, and recurring ideas found in the data (3-5 sentences).\n",
    "\n",
    "Step 2: Coding\n",
    "- Identify the most relevant codes (labels) in the text, each named in ≤3 words.\n",
    "- Present as a table with columns: | Code | Description |\n",
    "- The Description should clearly explain what the code represents.\n",
    "\n",
    "Step 3: Grouping\n",
    "- Organize the identified codes into coherent themes or groups based on conceptual similarity or relation.\n",
    "- Present as a table with columns: | Group # | Theme | Description |\n",
    "- The Description should briefly explain the theme and how it relates to the research question.\n",
    "\n",
    "Here is the full text to analyze:\n",
    "---\n",
    "{ text[len(text)//2:] }\n",
    "---\n",
    "\"\"\")\n",
    "\n",
    "for model_name in llm_models:\n",
    "    model = lms.llm(model_name)\n",
    "    prediction_stream = model.respond_stream(chat, config={\n",
    "        \"temperature\": 0.2,\n",
    "        \"topP\": 1.0,\n",
    "    })\n",
    "\n",
    "    for fragment in prediction_stream:\n",
    "        print(fragment.content, end=\"\", flush=True)\n",
    "\n",
    "    with open(f\"data/llm/{model_name}-response1-run1.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(prediction_stream.result().content)\n",
    "\n",
    "    # model.unload()\n",
    "    input(f\"Models: {llm_models}\\nCurrent model: {model_name}\\nLoad in the next model and press Enter to continue...\")\n",
    "    if input == \"exit\":\n",
    "        break\n",
    "\n",
    "print('Done! ✅', flush=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Given the text, give me the themes\n",
    "text = '\\n'.join(df['EXP'].astype(str))\n",
    "\n",
    "chat = lms.Chat(f\"\"\"You are an expert qualitative researcher conducting a thematic analysis to answer the Research Question: \"How do people experience their interaction with Artificial Social Agents?\"\n",
    "\n",
    "Please perform a full thematic analysis of the following text, following these three steps carefully:\n",
    "\n",
    "Step 1: Familiarization\n",
    "- Read the entire text thoroughly.\n",
    "- Provide a concise summary of the overall topics, key impressions, and recurring ideas found in the data (3-5 sentences).\n",
    "\n",
    "Step 2: Coding\n",
    "- Identify the most relevant codes (labels) in the text, each named in ≤3 words.\n",
    "- Present as a table with columns: | Code | Description |\n",
    "- The Description should clearly explain what the code represents.\n",
    "\n",
    "Step 3: Grouping\n",
    "- Organize the identified codes into coherent themes or groups based on conceptual similarity or relation.\n",
    "- Present as a table with columns: | Group # | Theme | Description |\n",
    "- The Description should briefly explain the theme and how it relates to the research question.\n",
    "\n",
    "Here is the full text to analyze:\n",
    "---\n",
    "{ text[:len(text)//2] }\n",
    "---\n",
    "\"\"\")\n",
    "\n",
    "for model_name in llm_models:\n",
    "    model = lms.llm(model_name)\n",
    "    prediction_stream = model.respond_stream(chat, config={\n",
    "        \"temperature\": 0.2,\n",
    "        \"topP\": 1.0,\n",
    "    })\n",
    "\n",
    "    for fragment in prediction_stream:\n",
    "        print(fragment.content, end=\"\", flush=True)\n",
    "\n",
    "    with open(f\"data/llm/{model_name}-response1-run2.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(prediction_stream.result().content)\n",
    "\n",
    "    # model.unload()\n",
    "    input(f\"Models: {llm_models}\\nCurrent model: {model_name}\\nLoad in the next model and press Enter to continue...\")\n",
    "    if input == \"exit\":\n",
    "        break\n",
    "\n",
    "print('Done! ✅', flush=True)"
   ],
   "id": "97b40119f9335aa4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "# Given the themes, can the LLM find them in the responses.\n",
    "# Analyze -> Identify -> Return\n",
    "chat = lms.Chat(\"\"\"You are an expert qualitative researcher conducting a thematic analysis, trying to answer the research question: \"How do people experience their interaction with Artificial Social Agents?\".\n",
    "Analyze the following themes and their definition:\n",
    "---\n",
    "Agent's Cognition: The agent is intelligent/knowledgeable.\n",
    "Agent's Coherence: The agent is perceived as logical and consistent.\n",
    "Agent's Creativeness: The agent is perceived as creative.\n",
    "Agent's Efficiency: The agent is perceived as efficient.\n",
    "Agent's Emotional Presence: The user's perception of the agent's emotions during and after interaction.\n",
    "Agent's Enjoyability: The extent to which the user finds the interaction with the agent enjoyable/boring.\n",
    "Agent's Helpfulness: The agent is perceived as helpful.\n",
    "Agent's Intentionality: The agent is perceived as acting deliberately and with intention.\n",
    "Agent's Interestingness: The extent to which the user finds interaction with the agent interesting.\n",
    "Agent's Intuitiveness: The extent to which the agent is perceived as intuitive.\n",
    "Agent's Limitation: The user perceives the agent as being useful only for limited use/purposes.\n",
    "Agent's Personality: The distinctive combination of character traits/qualities of the agent (or lack thereof).\n",
    "Agent's Quickness: The extent to which the agent performs tasks quickly.\n",
    "Agent's Reliability: The agent is perceived as reliable.\n",
    "Agent's Sociability: The user perceives the agent as sociable.\n",
    "Agent's Usability: The user perceives the agent as easy to use, user- or beginner-friendly, or simple to interact with.\n",
    "Attitude: The extent to which the user finds the interaction with the agent positive.\n",
    "Ease of Life: The agent is perceived as making the user's life easier.\n",
    "Emotional Experience: A self-contained emotional experience during interaction.\n",
    "Human-like Behaviour: The agent behaves like a human, expressively or emotionally, or conversely, like a machine/AI/tool.\n",
    "Limitations: User thoughts on things it cannot do well or problems/limitations noticed\n",
    "Performance: The extent to which the agent performs tasks well.\n",
    "Potential: The user perceives the agent having future potential for improvement.\n",
    "Productivity: The agent helps increase the user's productivity.\n",
    "User Acceptance: The likelihood that the user will use the agent again or in the future.\n",
    "User-Agent Alliance: The extent to which the user and agent collaborate for mutual benefit.\n",
    "User-Agent Interplay: The degree to which the user and agent influence each other.\n",
    "User's Autonomy: The user perceives the agent reducing the user's workload and allowing for more free time.\n",
    "User's Emotional Presence: The user's emotional state during and after interacting with the agent.\n",
    "User's Engagement: The extent to which the user feels involved in the interaction with the agent.\n",
    "User's Trust: The user perceives the agent as trustworthy and factual.\n",
    "---\n",
    "\n",
    "Analyze the following user responses and identify all applicable themes based on the provided definitions. Only assign a theme if there is clear and explicit support for it in the response - do not infer or assume. Give me a comma-separated list of all themes that you find in the following user-responses to an online questionnaire:\n",
    "---\n",
    "\"\"\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index >= 100:\n",
    "        break\n",
    "    chat.add_user_message(f\"\"\"User response {index}: {row['EXP']}\\n\"\"\")\n",
    "\n",
    "chat.add_user_message(f\"\"\"---\n",
    "Present as a table with columns: | Response # | Themes |\n",
    "Do not include any explanations or additional text outside of the table itself.\n",
    "\"\"\")\n",
    "\n",
    "for model_name in llm_models:\n",
    "    model = lms.llm(model_name)\n",
    "    prediction_stream = model.respond_stream(chat, config={\n",
    "        \"temperature\": 0.2,\n",
    "        \"topP\": 1.0,\n",
    "    })\n",
    "\n",
    "    for fragment in prediction_stream:\n",
    "        print(fragment.content, end=\"\", flush=True)\n",
    "\n",
    "    with open(f\"data/llm/{model_name}-response2.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(prediction_stream.result().content)\n",
    "\n",
    "    # model.unload()\n",
    "    input(f\"Models: {llm_models}\\nCurrent model: {model_name}\\nLoad in the next model and press Enter to continue...\")\n",
    "    if input == \"exit\":\n",
    "        break\n",
    "\n",
    "print('Done! ✅', flush=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c79a0d8a2043a1f6",
   "metadata": {},
   "source": [
    "# Analyze responses\n",
    "themes = [\n",
    "    \"Agent's Cognition\",\n",
    "    \"Agent's Coherence\",\n",
    "    \"Agent's Creativeness\",\n",
    "    \"Agent's Efficiency\",\n",
    "    \"Agent's Emotional Presence\",\n",
    "    \"Agent's Enjoyability\",\n",
    "    \"Agent's Helpfulness\",\n",
    "    \"Agent's Intentionality\",\n",
    "    \"Agent's Interestingness\",\n",
    "    \"Agent's Intuitiveness\",\n",
    "    \"Agent's Limitation\",\n",
    "    \"Agent's Personality\",\n",
    "    \"Agent's Quickness\",\n",
    "    \"Agent's Reliability\",\n",
    "    \"Agent's Sociability\",\n",
    "    \"Agent's Usability\",\n",
    "    \"Attitude\",\n",
    "    \"Ease of Life\",\n",
    "    \"Emotional Experience\",\n",
    "    \"Human-like Behaviour\",\n",
    "    \"Limitations\",\n",
    "    \"Performance\",\n",
    "    \"Potential\",\n",
    "    \"Productivity\",\n",
    "    \"User Acceptance\",\n",
    "    \"User-Agent Alliance\",\n",
    "    \"User-Agent Interplay\",\n",
    "    \"User's Autonomy\",\n",
    "    \"User's Emotional Presence\",\n",
    "    \"User's Engagement\",\n",
    "    \"User's Trust\",\n",
    "]\n",
    "\n",
    "\n",
    "def interpret_agreement(v):\n",
    "    if v <= 0:\n",
    "        return \"Poor agreement\"\n",
    "    elif 0 < v < 0.20:\n",
    "        return \"Slight agreement\"\n",
    "    elif 0.20 <= v < 0.40:\n",
    "        return \"Fair agreement\"\n",
    "    elif 0.40 <= v < 0.60:\n",
    "        return \"Moderate agreement\"\n",
    "    elif 0.60 <= v < 0.80:\n",
    "        return \"Substantial agreement\"\n",
    "    elif 0.80 <= v <= 1.00:\n",
    "        return \"Almost perfect agreement\"\n",
    "    else:\n",
    "        return \"Invalid value\"\n",
    "\n",
    "\n",
    "# kappa matrix:\n",
    "#         ...models...\n",
    "#  ...     k  k  k\n",
    "#  themes  k  k  k\n",
    "#  ...     k  k  k\n",
    "\n",
    "kappa_matrix = []\n",
    "metadata = {}\n",
    "\n",
    "for model in llm_models:\n",
    "    metadata[model] = {}\n",
    "\n",
    "    try:\n",
    "        with open(f\"data/llm/{model}-response2.md\", \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "    except FileNotFoundError:\n",
    "        with open(f\"data/llm/{model}-response2.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write('')\n",
    "            continue\n",
    "\n",
    "    if not text:\n",
    "        kappa_matrix.append([99 for theme in themes])\n",
    "        continue\n",
    "\n",
    "    matches = re.findall(r\"\\|? (.*) \\| (.*) \\|?\", text.replace(\"’\", \"'\"))\n",
    "    header = [m.strip() for m in matches[0]]\n",
    "    data = matches[1:]\n",
    "\n",
    "    df_ica = pd.DataFrame(data, columns=header)\n",
    "\n",
    "    kappa_vector = []\n",
    "\n",
    "    for theme in themes:\n",
    "        metadata[model][theme] = {}\n",
    "\n",
    "        kappa, a, b, c, d = 0, 0, 0, 0, 0\n",
    "        for idx, row in df_ica.iterrows():\n",
    "            theme_present = theme.lower() in df['Themes'][idx].lower()\n",
    "            peer_theme_present = theme.lower() in row['Themes'].lower()\n",
    "            if theme_present and peer_theme_present:\n",
    "                a += 1\n",
    "            elif theme_present and not peer_theme_present:\n",
    "                b += 1\n",
    "            elif not theme_present and peer_theme_present:\n",
    "                c += 1\n",
    "            else:\n",
    "                d += 1\n",
    "        N = (a + b + c + d)\n",
    "        P_o = (a + d) / N\n",
    "        P_e = ((a + b) / N) * ((a + c) / N) + ((c + d) / N) * ((b + d) / N)\n",
    "\n",
    "        metadata[model][theme]['a'] = a\n",
    "        metadata[model][theme]['b'] = b\n",
    "        metadata[model][theme]['c'] = c\n",
    "        metadata[model][theme]['d'] = d\n",
    "        if P_e == 1:\n",
    "            kappa_vector.append(99)\n",
    "            metadata[model][theme]['kappa'] = 99\n",
    "            metadata[model][theme]['interpret(k)'] = interpret_agreement(99)\n",
    "            continue\n",
    "        else:\n",
    "            kappa = (P_o - P_e) / (1.0 - P_e)\n",
    "            kappa_vector.append(kappa)\n",
    "            metadata[model][theme]['kappa'] = kappa\n",
    "            metadata[model][theme]['interpret(k)'] = interpret_agreement(kappa)\n",
    "    kappa_matrix.append(kappa_vector)\n",
    "\n",
    "with open(\"data/llm/metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "table = PrettyTable()\n",
    "col = [theme for theme in themes]\n",
    "table.add_column('Themes', col)\n",
    "for idx, model in enumerate(llm_models):\n",
    "    table.add_column(model, kappa_matrix[idx])\n",
    "    col = [\n",
    "        f\"k={metadata.get(llm_models[idx], {}).get(theme, {}).get('kappa', 99):.2f}    interpret(k)={metadata.get(llm_models[idx], {}).get(theme, {}).get('interpret(k)', 99)}     a={metadata.get(llm_models[idx], {}).get(theme, {}).get('a', 99)}, b={metadata.get(llm_models[idx], {}).get(theme, {}).get('b', 99)}, c={metadata.get(llm_models[idx], {}).get(theme, {}).get('c', 99)}, d={metadata.get(llm_models[idx], {}).get(theme, {}).get('d', 99)}\"\n",
    "        for theme in themes]\n",
    "    # table.add_column(model, col)\n",
    "\n",
    "print(table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe9a1286a213d980",
   "metadata": {},
   "source": [
    "avg_table = PrettyTable(['Model', 'Average Kappa', 'interpret(k)'])\n",
    "for idx, model in enumerate(llm_models):\n",
    "    # for each model: [list of kappa's from themes...]\n",
    "    avg_kappa = float(np.mean([k for k in kappa_matrix[idx] if k != 99]))\n",
    "    avg_table.add_row([model, avg_kappa, interpret_agreement(avg_kappa)])\n",
    "print(avg_table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c55082cc8d35308b",
   "metadata": {},
   "source": [
    "# Flatten kappa data into long form (theme, model, kappa)\n",
    "data = []\n",
    "for idx_model, model in enumerate(llm_models):\n",
    "    for idx_theme, theme in enumerate(themes):\n",
    "        kappa = metadata.get(model, {}).get(theme, {}).get('kappa', None)\n",
    "        if kappa is not None and kappa != 99:\n",
    "            data.append({'Theme': theme, 'Model': model, 'Kappa': kappa})\n",
    "\n",
    "label_map = {\n",
    "    'llama-3.1-8b-instruct': 'Llama',\n",
    "    'qwen3-32b': 'Qwen',\n",
    "    'deepseek-r1-distill-qwen-32b': 'DeepSeek',\n",
    "    'phi-4': 'Phi',\n",
    "    'gemma-3-12b': 'Gemma',\n",
    "    'mistral-nemo-instruct-2407': 'NeMo'\n",
    "}\n",
    "\n",
    "df_long = pd.DataFrame(data)\n",
    "df_long['Model'] = df_long['Model'].replace(label_map)\n",
    "print('Done! ✅', flush=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e52e4fa941b98b34",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.rcParams['figure.dpi'] = 600\n",
    "plt.ylim(-0.5,1)\n",
    "\n",
    "ax = sns.boxplot(data=df_long, x='Model', y='Kappa', fill=False, gap=.1,showmeans=True, meanline=True)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# plt.title(\"Kappa Distribution Across Models\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"data/img/llm-kappa-distributions-boxplot-model.png\", dpi=600)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56658985374f1203",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.rcParams['figure.dpi'] = 600\n",
    "plt.ylim(-0.5,1)\n",
    "\n",
    "ax = sns.boxplot(data=df_long, x='Theme', y='Kappa', fill=False, gap=.1,showmeans=True, meanline=True)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# plt.title(\"Kappa Distribution Across Themes\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"data/img/llm-kappa-distributions-boxplot-theme.png\", dpi=600)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
